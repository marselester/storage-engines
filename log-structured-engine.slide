Log-structured storage engine
15 Apr 2020

Marsel Mavletkulov
@marselester

* Log

Your favourite database probably uses *log-structured* (LSM) or *page-oriented* storage engine (B-tree)

In this presentation we'll look at ideas behind log-based key-value stores

Log is an append-only file to store sequence of records:

- appending a record to file is very efficient
- record lookup requires scanning entire file from beginning *O(n)*
- index based on hash map helps to locate the record faster *O(1)*

* Hash index

: Suppose an array can hold M key-value pairs, then a hash function should transform
: any key into an array index in the range [0; M-1]. It should uniformly distribute the keys.
: Modular hashing is the most commonly used method for hashing integers.

: Linear probing collision-resolution process: check the next entry in the table
: until finding either the search key or an empty entry.
: N/M (number of keys / array size) is a percentage of table entries that are occupied (load factor).
: It must not reach 1 (full table), should be between 1/8 and 1/2 (array is resized).
: Average number of compares for search is < 1.5, insert is < 2.5. Worst case is O(N).

: Separate chaining collision-resolution process: create linked-list of k/v for each array index.
: Items that collide are chained together.
: Since we have M lists (array size) and N keys, the average length of the lists is always N/M.
: The number of compares for search miss and insert is ~N/M.
: Average number of compares for search is N/(2*M), insert N/M. Worst case is O(N).

In-memory [[https://godoc.org/github.com/marselester/alg/search/hashtable][hash map]]:

- _key_ points to a byte offset in the log file where record is stored
- look up record's offset by _key_ from the hash map, then read the record from the log file
- when a new record is appended, the hash map points to the new offset

Riak's [[https://riak.com/assets/bitcask-intro.pdf][Bitcask]] offers fast reads/writes, but keys must fit in RAM

: On-disk hash map requires random access (slower than RAM)

Range queries in hash map are not efficient because each key has to be found individually

* Segment

If records are appended to one file forever, there will be no disk space left

The log is split into multiple files (segments):

- file is closed when it reaches certain size
- a new file is created to continue appending records
- duplicate keys are thrown away from the files (log compaction)
- compacted segments are merged into a new segment file (old segments are deleted)

* Key lookup in segments

Each segment has its own hash map

- start _key_ lookup from the most recent segment (using its hash map)
- check second-most-recent segment if _key_ wasn't found
- since segments are periodically merged (less files), search finishes quickly

I wrote [[https://github.com/marselester/rascaldb][RascalDB ðŸ˜œ]] in order to get a better grasp of the idea

* Deleting keys

Append â˜ ï¸ special deletion record (tombstone) to mark the _key_ for deletion

Old records associated with the _key_ will be discarded when segments are merged

* Concurrency

Since appends are sequential, there is only one writer

Multiple readers can read concurrently because segments are immutable

* Crash recovery

Partial writes:

- database might crash while appending a record
- Bitcask ignores records whose checksums don't match

Index is lost:

- Bitcask stores a snapshot of in-memory hash map on disk to speed up restarts
- corrupted index can be always created from segment files (slow restart)

* LSM storage engine (LevelDB, RocksDB)

* LSM-Tree

Log-Structured Merge-Tree (LSM-Tree) indexing structure:

- records are written into in-memory search tree
- the tree is periodically saved to a new segment file (keys are sorted)

* SSTable (segment)

Sorted String Table log format:

- records are stored in segment file sorted by _key_
- each _key_ apppears only once within a file

Advantages:

- efficient merging of segments because they are sorted by _key_ (check out k-way merge in [[https://github.com/marselester/hastydb][HastyDB ðŸš²]]) *O(n*log*k)*
- no need to hold all keys (byte offsets) in memory index, one _key_ for every few kilobytes is sufficient
- a group of records can be compressed into a block (less disk IO, saving disk space)
- _key_ in sparse index points to a byte offset in segment file where block is stored

* Memtable (tree index)

: Average cost of search/insert in red-black BST is O(lg N), worst case is O(2*lg N).

In-memory [[https://github.com/marselester/binary-search-tree][binary search tree]] (BST) is used to keep keys sorted:

- a new record is added into a balanced tree (memtable), e.g., [[https://godoc.org/github.com/marselester/alg/search/redblack-tree][red-black BST]] *O(lg*n)*
- the tree is written to disk in SSTable format when the tree is a few megabytes big
- SSTable is efficiently created from BST because it maintains keys in sorted order
- new records are added into the new memtable while SSTable is being written to disk

Range queries are efficient since data is stored in sorted order

* Key lookup

- start _key_ lookup from the memtable
- check the most recent segment (using its sparse index) if _key_ wasn't found

Often LSM storage engines speed up _key_ lookups with [[https://github.com/marselester/bloom][Bloom filters]] to avoid searching for non-existent keys

* Crash recovery

Memtable is lost:

- append every new write into a separate log file to restore memtable on database restart

* References

- [[https://dataintensive.net/][Designing Data-Intensive Applications]] Martin Kleppmann
- [[https://algs4.cs.princeton.edu][Algorithms]] Robert Sedgewick and Kevin Wayne
- [[https://www.databass.dev][Database Internals]] Alex Petrov
